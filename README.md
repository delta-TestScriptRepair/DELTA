# DELTA: Automated Repair of Deep Learning Test Scripts

**DELTA (DEep Learning library Test script repAir)** is the official implementation of our ICSE 2026 paper:
> _‚ÄúAutomated Test Script Repair of Deep Learning Library Testing‚Äù_

## 1.Description
Adequate testing of deep learning (DL) libraries is crucial for ensuring the reliability of both the libraries themselves and the DL software built upon them. Existing DL library testing techniques, despite their effectiveness, frequently generate invalid test scripts due to the inherent complexity of translating graph structures into executable code. These invalid test scripts can eventually lead to a high false-positive rate during testing, compromising fuzzing efficiency significantly. Yet, repairing invalid test scripts for DL library testing remains largely unexplored.

This paper presents ùê∑ùê∏ùêøùëáùê¥ (DEep Learning library Test script repAir). DELTA includes three key components to address challenges in repairing DL library test scripts: (1) Automated Error Analysis overcomes the difficulty of distinguishing errors by systematically parsing stack traces and runtime information to differentiate script-related failures from actual library bugs through execution context analysis; (2) Large Language Model-based Script Regeneration addresses the diverse error manifestations by prompting with categorized error characteristics and library-specific repair templates rather than raw error messages, enabling the generation of semantically correct repairs; and (3) Efficient Multi-Round Repair implements iterative refinement to address nested errors through progressive correction cycles. Our empirical evaluation across different fuzzing techniques and DL libraries shows that DELTA is able to repair 68-100% of invalid test scripts. The overall testing process with our repair technique is 2 to 13 times faster than re-testing, increasing testing throughput by up to 14.6% with little computational overhead.

You can access this repository using the following command:
-  `git clone https://github.com/delta-TestScriptRepair/DELTA.git`

DELTA automatically repairs invalid test scripts generated by DL testing tools (e.g., LEMON, MUFFIN, CEDAR), turning them into valid, executable test cases using a three-phase framework:
- **Automated Error Analysis**
- **LLM-based Script Regeneration**
- **Efficient Multi-Round Repair**

---
## 2.Framework version

We conduct our experiments under Python 3.12 using mainstream DL and LLM libraries, as shown belowÔºö

| Framework    | Version |
|--------------|---------|
| TensorFlow   | 2.19.0  |
| PyTorch      | 2.7.0   |
| Keras        | 3.9.0   |
| OpenAI SDK   | 0.28.0  |
| Tiktoken     | 0.8.0   |

---

## 3.Environment

-Update your OpenAI API key in run.py:
- `api_key = "sk-..."  # or load from env`

-Please install the packages our experiment require:
- `pip install -r requirements.txt`

---

## 4.File Structure

This project contains several folders related to preprocessing, repairing, and evaluating invalid test scripts in DL fuzzing.

- `input_files/`: The folder where you put the invalid models in, with .h5 and .pkl file of the same name (e.g. model1.h5,model1.pkl).
- `gpt_input/`: Models that failed in initial testing and require repair will be moved in to be repaired by LLM.
- `output_files/`: Successfully repaired test scripts that pass both model loading and prediction.
- `failure_files/`: Still-invalid test scripts after two rounds of repair. These are logged for further manual inspection or prompt refinement.

To run our framework (DELTA), the main program entry is `run.py`. It will execute the full pipeline: error classification ‚Üí input generation ‚Üí script repair ‚Üí multi-round testing.

We have provided demo files in the above folders, together with the repairing code in folder `repairs/` and `repairs2/`. We also provide the error_info.json and fail_error_info.json in our pipeline to show the exact error message.

**We are uploading the full dataset as fast as possible in zenodo.**

The core logic of DELTA is distributed across the following scripts:
- `api.py`: Defines repair prompt templates and calls GPT via OpenAI API.
- `code_process.py`: Implements the multi-round repair loop.
- `generated_input.py`: Helps the generation of input_generation.py.
- `input_generation.py`: Handles GPT-based generation of `.pkl` inputs for missing-input cases.
- `input_process.py`: Classifies errors, extracts messages, normalizes model format.
- `test_model.py`: Validates model predictability using generated inputs.
- `namedel.py`: Helps rename the file names and clears .pkl files with no related .h5 files.
Besides, we provide our code to transform MUFFIN's models and inputs to .h5 and .pkl files:
- ` mfh5.py`: Generates .h5 model file through MUFFIN's models.
- ` mfpkl.py`: Generates .pkl model input file through MUFFIN's inputs.
- ` layer_map.py`: Helps with mfh5.py

---

## 5.Experiments

### 5.1 Main Method

**Step 0**: If you want to repair MUFFIN's files:
```bash
mkdir muffin_files
python mfh5.py mfpkl.py
```

If you want to repair LEMON's files:
```bash
python namedel.py
```

**Step 1**: Make sure you under repaired files in .input_files/:
```bash
python run.py
```

### 5.2 Evaluation 

Repair Results across DL Testing Tools

| Tool   | LLM Version     | Invalid | Repaired | Failed | Repair Rate | Repair Scripts |
|--------|------------------|---------|----------|--------|--------------|----------------|
| LEMON  | gpt-3.5-turbo    | 49      | 48       | 1      | 97.96%       | 14             |
| LEMON  | gpt-4-turbo      | 49      | 49       | 0      | 100.00%      | 13             |
| MUFFIN | gpt-3.5-turbo    | 139     | 94       | 45     | 67.63%       | 24             |
| MUFFIN | gpt-4-turbo      | 139     | 106      | 33     | 76.26%       | 23             |
| CEDAR  | gpt-3.5-turbo    | 63      | 56       | 7      | 88.89%       | 17             |
| CEDAR  | gpt-4-turbo      | 63      | 57       | 6      | 90.48%       | 17             |

Repair Efficiency across DL Testing Tools

| Tool   | LLM Version     | Original Valid Cases | Generation Time (s) | Repaired Cases | Repair Time (s) |
|--------|------------------|----------------------|----------------------|----------------|-----------------|
| LEMON  | gpt-3.5-turbo    | 701                  | 37446                | 48             | 247             |
| LEMON  | gpt-4-turbo      | 701                  | 37446                | 49             | 203             |
| MUFFIN | gpt-3.5-turbo    | 607                  | 9627                 | 94             | 157             |
| MUFFIN | gpt-4-turbo      | 607                  | 9627                 | 106            | 237             |
| CEDAR  | gpt-3.5-turbo    | 1822                 | 15176                | 56             | 175             |
| CEDAR  | gpt-4-turbo      | 1822                 | 15176                | 57             | 194             |

Prompt Reuse Ratio (ùê∑ùê∏ùêøùëáùê¥)

| Tool   | LLM Version     | Repaired Scripts | Unique Prompts | Reuse Ratio |
|--------|------------------|------------------|----------------|-------------|
| LEMON  | gpt-3.5-turbo    | 48               | 14             | 3.43        |
| LEMON  | gpt-4-turbo      | 49               | 13             | 3.77        |
| MUFFIN | gpt-3.5-turbo    | 94               | 24             | 3.92        |
| MUFFIN | gpt-4-turbo      | 106              | 23             | 4.61        |
| CEDAR  | gpt-3.5-turbo    | 56               | 17             | 3.29        |
| CEDAR  | gpt-4-turbo      | 57               | 17             | 3.35        |
